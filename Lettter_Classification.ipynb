{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lettter Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "udisgjcUjvjn",
        "3hfV7jhOavBP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKnRXs-iavBB",
        "colab_type": "text"
      },
      "source": [
        "# Classification of the Latin alphabet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnF4KOGyavBC",
        "colab_type": "text"
      },
      "source": [
        "### Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC5moJH_avBD",
        "colab_type": "text"
      },
      "source": [
        "For this project we were asked to classify a letter given a string of 100 zeros and ones using persistent homology. We were told that the algorithm should be able to compute a vector of features that we can use to compare to the other features of the Latin alphabet. The features that my group decided to use were probing from the upper left to lower right, upper right to lower left, bottom right to the upper left, scanning from left to right, scanning from right to left, and homology of 1 dimension. Below are pictures of each of the scans:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxldmFcgnOMM",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=16ItJF8t6IQ_0TVW4zZ7Tl324fbCQt0hH)\n",
        "![alt text](https://drive.google.com/uc?id=1QIxpbakPRpG65r9n37-eKGqIiJj81mdc)\n",
        "![alt text](https://drive.google.com/uc?id=1fnV8PKe2kDIGk2-qcI9JsReK4XYQme1d)\n",
        "![alt text](https://drive.google.com/uc?id=1fnWEwOQSX5uls76EgrM6JgzyxaM5VJtO)\n",
        "![alt text](https://drive.google.com/uc?id=1HzEaNr_AX8HOLy3AYagcq_8XdmHy8Nsi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyDhYU6_avBE",
        "colab_type": "text"
      },
      "source": [
        "### Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zbr_2rPqURC",
        "colab_type": "text"
      },
      "source": [
        "In this work, we are going to scan all 26 letters of the Latin alphabet in different ways by using persistent homology. Persistent homology is a method for computing change of topological features of a space. With Lower Star Image Filtrations, we were able to find the life and death pair for each single letter in 0-dimensional homology. Lower Star Image Filrations allows us to express our local minimums as birth times and our saddle points as death times. This is useful in our project since it gives us a way to summarize the critical points of our image. Next, we convert all the pairs to feature vector for each letter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "udisgjcUjvjn"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGFTkX3Yj53B",
        "colab_type": "code",
        "outputId": "65cf1128-a57e-43c3-aa05-fda8eaf0fec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "pip install scikit-tda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-tda\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/67/14676df9d3c8fae3324f6a73e60e9b705c1dfea8131e275a313ab90bd09d/scikit_tda-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (1.17.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (4.3.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (0.3.10)\n",
            "Collecting persim\n",
            "  Downloading https://files.pythonhosted.org/packages/71/4a/ac537e6743337b00a8b3ff8b0d967827d3b5cfd9afd3a0bd117f5809d4d2/persim-0.1.1-py3-none-any.whl\n",
            "Collecting ripser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/17/d5e898161eb8e3b1084419bfc04adbe87d15075daa8e236df72dbe75167e/ripser-0.4.1.tar.gz (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (1.3.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (0.40.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (3.1.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (0.29.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (0.21.3)\n",
            "Collecting tadasets\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/06/d1b9edccfcd071b245b0d1ab4b22eb2ff7aaeaa6d015db58d701d9782122/tadasets-0.0.4-py3-none-any.whl\n",
            "Collecting kmapper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/3f/a1290b9425a7e0ff4ae51a6e6ff68e50ad793b3460f435c2ec81c0383751/kmapper-1.2.0-py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->scikit-tda) (0.46)\n",
            "Collecting hopcroftkarp\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/56/7b03eba3c43008c490c9d52e69ea5334b65955f66836eb4f1962f3b0d421/hopcroftkarp-1.2.5.tar.gz\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->scikit-tda) (0.30.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-tda) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-tda) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-tda) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-tda) (0.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->scikit-tda) (0.14.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from kmapper->scikit-tda) (2.10.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->scikit-tda) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->scikit-tda) (41.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2->kmapper->scikit-tda) (1.1.1)\n",
            "Building wheels for collected packages: ripser, hopcroftkarp\n",
            "  Building wheel for ripser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ripser: filename=ripser-0.4.1-cp36-cp36m-linux_x86_64.whl size=423338 sha256=ea82fcdb022a34de8233c56865b5166f2931ba5f8d44af8917069a6a72eabda9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/12/da/56d55c3af62ebd5e0684d521f3e58c1a85ac312502c9e2d47d\n",
            "  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopcroftkarp: filename=hopcroftkarp-1.2.5-py2.py3-none-any.whl size=18091 sha256=6fb2ddfc8987884be5fa61b13fd8599e2e09864d357260f67fef176a429d15ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/e1/c9/1993c7f7f114b7d3fb2d3e895e02157a7ebf554861e9e54e01\n",
            "Successfully built ripser hopcroftkarp\n",
            "Installing collected packages: hopcroftkarp, persim, ripser, tadasets, kmapper, scikit-tda\n",
            "Successfully installed hopcroftkarp-1.2.5 kmapper-1.2.0 persim-0.1.1 ripser-0.4.1 scikit-tda-0.0.3 tadasets-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtHm743sj_Bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import Series, DataFrame\n",
        "from numpy import genfromtxt\n",
        "from ripser import ripser,lower_star_img\n",
        "from persim import plot_diagrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfH68c7okFlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded= files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2SGNSaavBF",
        "colab_type": "text"
      },
      "source": [
        "### First part of the algrorithm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUJXkum7avBG",
        "colab_type": "text"
      },
      "source": [
        "For all five scans, we use the lower_star_img package from ripser, to obtain the coordinates of each of the 0-order life-death pairs. At the additional last test, we run 1st order homology test for each letters and also obtain their coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OKMR1ZbM0VS",
        "colab_type": "code",
        "outputId": "c3c3fb11-e457-4261-e656-c319b285610d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "letters = genfromtxt('Letters.csv', delimiter =',')\n",
        "#the main block: run 6 scans for each of the 26 letters, store results in a dictionary.\n",
        "i=0\n",
        "i_to_result = dict()\n",
        "while i<=25:\n",
        "    letter_one_line = letters[i,:]\n",
        "#initialize matrix of size 10x10 with all values 100\n",
        "    letter= np.full((10,10),100)\n",
        "   #test 1 probing upper left\n",
        "    for k in range(1,100):\n",
        "        if letter_one_line[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max((k-1)%10, int((k-1)/10))\n",
        "    PULscan = lower_star_img(letter)\n",
        "    #test 2 left to right\n",
        "    for j in range(1,100):\n",
        "        if letter_one_line[j]==1.0:\n",
        "            row=int((j-1)/10)\n",
        "            column= (j-1)%10\n",
        "            letter[row,column]= j%10\n",
        "    LRscan = lower_star_img(letter)\n",
        "    #test 3 right to left\n",
        "    for k in range(1,100):\n",
        "        if letter_one_line[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= 10 - (k-1)%10\n",
        "    RLscan = lower_star_img(letter)\n",
        "    #test 4 probing from bottom right\n",
        "    for k in range(1,100):\n",
        "        if letter_one_line[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10 \n",
        "            letter[row,column]= max(9-(k-1)%10,9-int((k-1)/10))  \n",
        "    PLBscan = lower_star_img(letter)\n",
        "    #test 5 probing from upper right to lower left \n",
        "    for k in range(1,100):       \n",
        "        if letter_one_line[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max(9-(k-1)%10,int((k-1)/10))  \n",
        "    PURscan = lower_star_img(letter) \n",
        "    #test 6\n",
        "    letter_ones_line = letters[i,1:]\n",
        "    newletter = letter_ones_line.reshape(10,10)\n",
        "    coordinates = np.argwhere(newletter == 1)\n",
        "    dgms = ripser(coordinates)['dgms']\n",
        "    h1_test = dgms[1]\n",
        "    \n",
        "    i_to_result[i] = (PULscan, LRscan, RLscan, PLBscan, PURscan, h1_test)\n",
        "    i+=1\n",
        "\n",
        "#change all infinity in life-death pair to 100.    \n",
        "l=0\n",
        "while l <=25:\n",
        "    for j in range(6):\n",
        "        len_j = len(i_to_result[l][j])\n",
        "        for k in range(len_j):\n",
        "            if str(i_to_result[l][j][k][1])=='inf':\n",
        "                i_to_result[l][j][k][1]=100\n",
        "    l+=1\n",
        "#calculate feature vector for each letter, store them in a matrix. \n",
        "#every feature vector has 5 components, each component stands for the sum of \"lifespan\" of the life-death pairs in one scan.\n",
        "l=0\n",
        "vec_mtx = np.zeros((26,6))\n",
        "while l < 26:\n",
        "    for j in range(6):\n",
        "        len_j = len(i_to_result[l][j])\n",
        "        sum_j = 0\n",
        "        for k in range(len_j):\n",
        "            sum_j = sum_j+ (i_to_result[l][j][k][1]-i_to_result[l][j][k][0])\n",
        "            vec_mtx[l][j] = sum_j\n",
        "    l+=1\n",
        "print('feature matrix:\\n',vec_mtx,'\\n')\n",
        "\n",
        "#calculate the pairwise distance between each two letter, store them in a list.\n",
        "all_dis = []\n",
        "for q in range(26):\n",
        "    for p in range(q):\n",
        "            dis = np.linalg.norm(vec_mtx[q]-vec_mtx[p])\n",
        "            all_dis.append((dis,p,q))\n",
        "\n",
        "#sort all the distance from low to high, find the minimum distance, also find which letter pair has the minimum distance.             \n",
        "def getkey(item):\n",
        "    return item[0]\n",
        "\n",
        "sorted_dis = sorted(all_dis, key=getkey)\n",
        "print('how many items in this sorted distance list:',len(sorted_dis),'\\n','sorted distance:',sorted_dis,'\\n')\n",
        "\n",
        "print('min distance is between',sorted_dis[0][1],'th letter and',sorted_dis[0][2],'th letter. min distance is',sorted_dis[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature matrix:\n",
            " [[ 97.          98.          97.          98.          97.\n",
            "    3.48528123]\n",
            " [ 98.          97.          98.          98.          97.\n",
            "    6.23334539]\n",
            " [ 98.          98.         102.          98.          99.\n",
            "    0.82842708]\n",
            " [ 98.          97.          98.          98.          98.\n",
            "    5.24264061]\n",
            " [ 98.          97.         107.         100.         101.\n",
            "    0.73304415]\n",
            " [ 98.          97.         100.          95.         100.\n",
            "    0.3245554 ]\n",
            " [ 98.          98.         100.          97.          99.\n",
            "    2.42366862]\n",
            " [ 98.          97.          96.          97.          97.\n",
            "    0.12310553]\n",
            " [ 97.          97.          96.          96.          96.\n",
            "    0.        ]\n",
            " [ 97.          98.          95.          96.          96.\n",
            "    0.        ]\n",
            " [ 98.          97.          99.          97.          98.\n",
            "    0.82842708]\n",
            " [ 98.          97.          96.          97.          93.\n",
            "    0.        ]\n",
            " [ 99.          98.          97.         100.          98.\n",
            "    2.12899017]\n",
            " [ 99.          98.          96.          97.          97.\n",
            "    0.82842708]\n",
            " [ 98.          98.          97.          99.          99.\n",
            "    4.41673815]\n",
            " [ 98.          97.          97.          96.          98.\n",
            "    2.57649124]\n",
            " [ 99.          99.          97.          99.         100.\n",
            "    4.41421354]\n",
            " [ 99.          97.          98.          97.          97.\n",
            "    3.24264061]\n",
            " [ 98.         102.         102.          98.         100.\n",
            "    0.57076645]\n",
            " [ 99.          98.          97.          95.          98.\n",
            "    0.        ]\n",
            " [ 99.          97.          97.          98.          99.\n",
            "    0.29705858]\n",
            " [100.          98.          97.          96.          98.\n",
            "    0.41421354]\n",
            " [100.          98.          98.          99.         101.\n",
            "    2.60112619]\n",
            " [ 98.          98.          98.          97.          97.\n",
            "    2.3071357 ]\n",
            " [ 98.          97.          96.          96.          97.\n",
            "    0.47213602]\n",
            " [101.         102.         101.          99.          98.\n",
            "    0.82842708]] \n",
            "\n",
            "how many items in this sorted distance list: 325 \n",
            " sorted distance: [(1.0591611238255987, 7, 24), (1.4076561905961278, 1, 3), (1.4142135623730951, 8, 9), (1.4736257514047435, 19, 21), (1.4909434670721213, 8, 24), (1.5803412572654365, 7, 13), (1.6956324610199927, 17, 23), (1.7320526474901206, 14, 16), (1.7364201595594493, 7, 8), (1.7683165204106095, 13, 24), (2.0180565910073907, 15, 23), (2.042442864611687, 13, 21), (2.0549726085777977, 9, 24), (2.094761772796275, 0, 23), (2.1080215828055, 15, 17), (2.163819511130015, 3, 14), (2.239454167989259, 7, 9), (2.2774062433337536, 10, 23), (2.2853692091224644, 19, 24), (2.3547389647468817, 6, 10), (2.535411381850512, 15, 24), (2.5857864221185745, 13, 19), (2.6206129054684464, 0, 14), (2.6457513110645907, 3, 17), (2.646385273156172, 21, 24), (2.6483438756482296, 20, 21), (2.6514636836676537, 7, 20), (2.656854242861205, 0, 17), (2.6623884032520984, 0, 3), (2.6807795875781286, 13, 23), (2.698583420324775, 10, 20), (2.698583420324775, 13, 20), (2.6994973083127967, 16, 22), (2.746779130563143, 2, 6), (2.772416170204751, 8, 13), (2.772416170204751, 9, 13), (2.7974808757458254, 0, 15), (2.8308267205643594, 6, 23), (2.8311049027763815, 7, 19), (2.8382614940645636, 10, 15), (2.8382614940645636, 13, 15), (2.843368402927536, 7, 21), (2.8694582792174432, 12, 14), (2.9712669028349907, 10, 17), (3.003181886857116, 7, 15), (3.04202016252468, 5, 10), (3.058753562192961, 12, 20), (3.0605920713981494, 23, 24), (3.104562302921846, 15, 19), (3.1105377100275295, 15, 21), (3.1256979710496107, 7, 23), (3.167120475987734, 20, 24), (3.1892903372385515, 10, 21), (3.209370943505916, 1, 14), (3.2258760570274823, 5, 6), (3.2399812483122954, 7, 10), (3.248362145664116, 0, 1), (3.253176035079121, 21, 23), (3.266606055667772, 6, 17), (3.268989357708706, 10, 19), (3.290657534275276, 13, 17), (3.292343455308958, 0, 12), (3.3082193171478025, 1, 17), (3.3166247903554, 8, 11), (3.3166247903554, 8, 19), (3.3299014702122336, 19, 20), (3.335707318749216, 10, 24), (3.3460145286381366, 15, 20), (3.3499620635411715, 12, 16), (3.408399785263611, 3, 23), (3.418521818098106, 3, 16), (3.4641016151377544, 2, 10), (3.4641016151377544, 10, 13), (3.4674709445448895, 6, 15), (3.4686792629312486, 12, 23), (3.4699293975265806, 20, 23), (3.479705807229299, 3, 15), (3.4961282044577597, 12, 22), (3.5066289772175394, 14, 22), (3.5103952973500068, 19, 23), (3.518309967686826, 14, 17), (3.555039675267126, 8, 15), (3.5625081570823873, 12, 13), (3.6055512374908316, 17, 21), (3.605551275463989, 9, 11), (3.605551275463989, 9, 19), (3.613706405978494, 0, 13), (3.638710940033236, 12, 17), (3.648112876441578, 20, 22), (3.650051389180301, 8, 23), (3.667481765821636, 14, 23), (3.6980664818982665, 17, 24), (3.792955142257148, 14, 15), (3.830985970796699, 17, 20), (3.8381635150976563, 7, 17), (3.8660836888356886, 3, 6), (3.88057650209476, 0, 10), (3.8815446666107274, 18, 25), (3.8865584011749648, 5, 19), (3.895070327375771, 8, 21), (3.905622443248538, 2, 5), (3.9120615355808326, 0, 7), (3.96059230681634, 8, 10), (3.982827542711841, 0, 16), (4.001893922947541, 7, 11), (4.003934463688903, 6, 22), (4.00986833283281, 0, 24), (4.015846277600932, 0, 6), (4.064784159713256, 6, 20), (4.079008101576092, 9, 15), (4.085518861696584, 10, 12), (4.085929400980456, 3, 12), (4.119748311432924, 6, 14), (4.125963307950302, 7, 12), (4.131148629484805, 2, 18), (4.143859656793796, 9, 21), (4.150049689100727, 11, 24), (4.162075821469023, 9, 23), (4.1731430020562055, 1, 23), (4.185058918924211, 17, 19), (4.253027604109994, 8, 20), (4.2661759459376, 12, 15), (4.322764326309356, 11, 13), (4.359820934656835, 5, 21), (4.36324532175667, 6, 21), (4.367060226962038, 5, 15), (4.375749678095554, 0, 8), (4.37775783307109, 0, 20), (4.3942281110607855, 1, 16), (4.401429573896292, 1, 15), (4.401429667216539, 16, 17), (4.465474096725329, 12, 21), (4.467915516146843, 6, 16), (4.4689774849631165, 14, 20), (4.47222048593539, 5, 20), (4.481833931403168, 6, 12), (4.521037160999228, 16, 23), (4.529317625743097, 8, 17), (4.568825844128019, 6, 19), (4.598606880931805, 0, 9), (4.623550935954804, 15, 16), (4.6352218027363135, 3, 10), (4.68301448453738, 5, 23), (4.688027136627815, 3, 22), (4.699813166639882, 9, 20), (4.705577771300183, 10, 22), (4.734082883909135, 17, 22), (4.769189206065441, 12, 24), (4.782883688667644, 10, 14), (4.782883688667644, 13, 14), (4.790716536239136, 16, 20), (4.815244458713966, 2, 23), (4.840604999561364, 0, 21), (4.852297970250509, 6, 13), (4.866856420813416, 9, 10), (4.876739378337856, 21, 22), (4.899041424719925, 15, 22), (4.90779282466864, 22, 23), (4.951124828199568, 1, 6), (4.984549301523895, 1, 12), (5.043138553163, 6, 18), (5.080204670865097, 6, 24), (5.101154775192579, 5, 24), (5.113431846162941, 0, 19), (5.127630111784468, 6, 7), (5.130582339623548, 11, 23), (5.149244425628588, 9, 17), (5.149293285146158, 5, 17), (5.22325114047293, 2, 20), (5.240112750433508, 3, 20), (5.242640666740577, 3, 13), (5.270837706280598, 0, 22), (5.278054996856731, 13, 16), (5.291502622129181, 2, 25), (5.304946951832261, 2, 22), (5.304946951832261, 13, 22), (5.332474203560016, 7, 14), (5.369211022848948, 2, 17), (5.388931438657519, 5, 7), (5.434390412748942, 12, 19), (5.464235037939374, 10, 16), (5.479069564109438, 14, 21), (5.52563110949126, 8, 12), (5.528099671545803, 14, 24), (5.5677643628300215, 16, 21), (5.584153883409815, 5, 22), (5.590517567203119, 5, 13), (5.613797124484201, 11, 17), (5.635398307124009, 3, 24), (5.656854249492381, 11, 19), (5.669848785051797, 0, 11), (5.6719990175601405, 11, 21), (5.67535369142605, 3, 7), (5.675662249507845, 1, 10), (5.703735544200627, 9, 12), (5.71764500203349, 2, 12), (5.771802840666905, 3, 21), (5.791786908381317, 6, 25), (5.799854057881404, 11, 15), (5.81083965336458, 19, 22), (5.820152024988017, 6, 8), (5.8317319814199005, 2, 4), (5.835728601356442, 2, 15), (5.839977414831645, 5, 8), (5.845645632024165, 2, 21), (5.889506891142312, 10, 11), (5.927949597557555, 7, 22), (5.932370207866713, 1, 22), (6.017735618194642, 1, 13), (6.056920952168088, 2, 19), (6.087600018776694, 0, 2), (6.116666410093156, 7, 16), (6.122522450800983, 2, 3), (6.1243428959998765, 14, 19), (6.1693289653006715, 5, 18), (6.175958400260999, 22, 25), (6.207463182184621, 22, 24), (6.220246327058001, 12, 25), (6.235060254666591, 2, 14), (6.244212343040575, 0, 5), (6.250311112168272, 10, 18), (6.252059164892923, 11, 20), (6.283731424313536, 3, 19), (6.288081993267387, 16, 24), (6.362804461313063, 3, 8), (6.362804504348855, 16, 19), (6.3645562223759296, 8, 14), (6.417677087324943, 23, 25), (6.417753672925978, 3, 5), (6.418063055943195, 1, 24), (6.421798895611531, 1, 20), (6.4445790520543405, 11, 12), (6.469765416909976, 2, 16), (6.47102538968687, 6, 9), (6.48074069840786, 2, 13), (6.48074069840786, 10, 25), (6.4901741651684945, 18, 22), (6.493964340462857, 21, 25), (6.506537572951515, 1, 7), (6.51900901130036, 2, 7), (6.519783424914063, 9, 14), (6.576928223870398, 5, 12), (6.614072852952884, 5, 14), (6.6411848495388, 5, 9), (6.697932181024764, 1, 21), (6.709320247238359, 18, 23), (6.717659065206948, 2, 24), (6.771843511913452, 16, 25), (6.818011485242996, 3, 9), (6.835590955532766, 5, 16), (6.8762164361261755, 20, 25), (6.987734039578398, 17, 25), (6.989606193708787, 1, 8), (7.0710678118654755, 13, 25), (7.086123197524049, 1, 2), (7.125453355923639, 16, 18), (7.189317868950627, 2, 8), (7.201013452503212, 1, 19), (7.2162951710885785, 18, 20), (7.240722420806826, 12, 18), (7.258532318644919, 19, 25), (7.281792966974647, 18, 21), (7.313363190800938, 8, 16), (7.313363190800938, 9, 16), (7.40039576489759, 8, 22), (7.406388778826847, 1, 9), (7.407832097133615, 14, 25), (7.417973716011397, 4, 18), (7.4332958147248975, 5, 25), (7.438129760852192, 18, 19), (7.544123486097521, 1, 5), (7.553732454132196, 0, 25), (7.581904814286804, 3, 11), (7.602071977056552, 14, 18), (7.6658892163345405, 9, 22), (7.672950514239754, 6, 11), (7.690182803838489, 17, 18), (7.713261074480635, 0, 18), (7.736575130064481, 1, 11), (7.842676578042994, 11, 14), (7.875463918020496, 15, 18), (7.8775458303135, 15, 25), (7.9414349458327855, 13, 18), (7.967765129605797, 3, 25), (7.980369128105089, 2, 9), (8.03103221817814, 7, 25), (8.115307210271776, 4, 6), (8.174742088811922, 3, 18), (8.312961939389817, 5, 11), (8.314261441424433, 24, 25), (8.438032964025735, 7, 18), (8.60289067473192, 18, 24), (8.61470498453014, 1, 25), (8.642123085261675, 2, 11), (8.669882528572723, 4, 5), (8.859191902231382, 11, 16), (8.926717841447257, 8, 25), (8.94478048378582, 4, 25), (9.038046880870489, 9, 25), (9.055887471866889, 4, 10), (9.058962427130702, 1, 18), (9.073355186438757, 8, 18), (9.152368954380838, 11, 22), (9.291166468171534, 9, 18), (9.417339933378893, 11, 25), (9.512609028678119, 4, 22), (10.44940588804216, 4, 20), (10.463162247589974, 4, 23), (10.551102991596428, 11, 18), (10.627730957597764, 4, 12), (10.644156820869837, 4, 17), (10.692822840360837, 3, 4), (10.934788590666265, 4, 14), (10.979572308740792, 4, 16), (11.331297241298651, 4, 15), (11.383093118873102, 0, 4), (11.406211156972343, 4, 21), (11.45658386044323, 1, 4), (11.684919927931533, 4, 19), (12.098430688400168, 4, 7), (12.165898976364865, 4, 13), (12.372068260830382, 4, 24), (12.78817241525041, 4, 8), (13.694427834786362, 4, 9), (13.947664812511501, 4, 11)] \n",
            "\n",
            "min distance is between 7 th letter and 24 th letter. min distance is 1.0591611238255987 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flyYvM5QavBK",
        "colab_type": "text"
      },
      "source": [
        "The algorithm above obtains a feature vector for all the letters of the Latin alphabet. By using a sorted list we notice that the smallest distance is between letter H and W. Thus, we have classified all of our letters since the minimum distance is above 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfV7jhOavBP",
        "colab_type": "text"
      },
      "source": [
        "### Our experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R-kfIXttzIUg"
      },
      "source": [
        "Next, we will create a test feature block which will contain all the features we have already implented ie. Probing from upper left, scanning from left to right, scanning from right to left, etc. This will allow us to now test a new string of 100 zeros and ones against our feature matrix so that we can compare our test to our feature matrix and find out the best fitted letter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImtdLt3mavBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert new test input list of 01010101... into feature vector.\n",
        "def get_feature_vec(test): # 'test' stands for the 01010101... input.\n",
        "    #run all the scans, get the life-death pairs.\n",
        "    letter= np.full((10,10),100)\n",
        "   #test 1 probing upper left\n",
        "    for k in range(1,100):\n",
        "        if test[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max((k-1)%10, int((k-1)/10))\n",
        "    PULscan = lower_star_img(letter)\n",
        "    #test 2 left to right\n",
        "    for j in range(1,100):\n",
        "        if test[j]==1.0:\n",
        "            row=int((j-1)/10)\n",
        "            column= (j-1)%10\n",
        "            letter[row,column]= j%10\n",
        "    LRscan = lower_star_img(letter)\n",
        "    #test 3 right to left\n",
        "    for k in range(1,100):\n",
        "        if test[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= 10 - (k-1)%10\n",
        "    RLscan = lower_star_img(letter)\n",
        "    #test 4 probing from bottom right\n",
        "    for k in range(1,100):\n",
        "        if test[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max(9-(k-1)%10,9-int((k-1)/10))  \n",
        "    PLBscan = lower_star_img(letter)\n",
        "    #test 5 probing from upper right to lower left \n",
        "    for k in range(1,100):       \n",
        "        if test[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max(9-(k-1)%10,int((k-1)/10))  \n",
        "    PURscan = lower_star_img(letter) \n",
        "    #test 6\n",
        "    newletter = test[1:].reshape(10,10)\n",
        "    coordinates = np.argwhere(newletter == 1)\n",
        "    dgms = ripser(coordinates)['dgms']\n",
        "    h1_test = dgms[1]\n",
        "    test_result = (PULscan, LRscan, RLscan, PLBscan, PURscan, h1_test)\n",
        "    #change all infini to 100\n",
        "    for j in range(6):\n",
        "        len_j = len(test_result[j])\n",
        "        for k in range(len_j):\n",
        "            if str(test_result[j][k][1])=='inf':\n",
        "                test_result[j][k][1]=100\n",
        "    #get the feature vector\n",
        "    test_feature = np.zeros(6)\n",
        "    for j in range(6):\n",
        "        len_j = len(test_result[j])\n",
        "        sum_j = 0\n",
        "        for k in range(len_j):\n",
        "            sum_j = sum_j+ (test_result[j][k][1]-test_result[j][k][0])\n",
        "            test_feature[j] = sum_j\n",
        "            \n",
        "    return test_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SofKsq3FavBS",
        "colab_type": "text"
      },
      "source": [
        "To determine that we do get the correct feature vector we first take a known string of zeros and ones. We will compare this to our feature matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA6wLb166jAP",
        "colab_type": "code",
        "outputId": "4ebead36-3bbd-4bfa-8e58-311a64b9ea0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "#Here is where we make up a test list. We will be using the letter I (the 8th letter) in 10X10 grid.\n",
        "New_I = [8,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "New_I = np.asarray(New_I)\n",
        "plt.imshow(New_I[1:].reshape(10,10))\n",
        "print(New_I)\n",
        "feature_vect = get_feature_vec(New_I)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[8])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "feature vect of input list is: [97. 97. 96. 96. 96.  0.]\n",
            "standard feature vect of I is: [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 8 th letter \n",
            " the closest distance is: 0.0 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAJgElEQVR4nO3dzYtdBx2H8edrJk1MFFvQTZNis5BK\nEGxlqNWCi6bgS4vduGihgt1kY18UobRu+g+I6KIUQtWNxS7SLkopVtF24SY4TQNtEoVSa5u+YFz4\nQsEkpT8XM0JMmtwzN/d4Zn4+HwjkvuTmS5gn596TOzepKiT18aGpB0haLKOWmjFqqRmjlpoxaqmZ\npTEe9LJsq+3sHOOhJQH/4l1O16l80G2jRL2dnXw++8Z4aEnAofrNBW/z6bfUjFFLzRi11IxRS80Y\ntdSMUUvNDIo6yVeS/DHJK0keGHuUpPnNjDrJFuBh4KvAXuCOJHvHHiZpPkOO1NcDr1TVq1V1Gngc\nuG3cWZLmNSTqXcAbZ10+sXbdf0myP8lKkpUznFrUPknrtLATZVV1oKqWq2p5K9sW9bCS1mlI1G8C\nV511effadZI2oCFR/x74VJI9SS4DbgeeGneWpHnN/C6tqnovyd3As8AW4KdVdXT0ZZLmMuhbL6vq\nGeCZkbdIWgDfUSY1Y9RSM0YtNWPUUjNGLTUzygcPCp5968jUEzaEL1957dQT/u94pJaaMWqpGaOW\nmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aa\nMWqpGaOWmvHTREcy1qdojvUppX7qZx8eqaVmjFpqxqilZoxaasaopWaMWmrGqKVmZkad5KokzyU5\nluRokvv+F8MkzWfIm0/eA75XVYeTfBR4Icmvq+rYyNskzWHmkbqq3q6qw2s//ydwHNg19jBJ81nX\n20STXA1cBxz6gNv2A/sBtrNjAdMkzWPwibIkHwGeAL5TVf849/aqOlBVy1W1vJVti9woaR0GRZ1k\nK6tBP1ZVT447SdKlGHL2O8BPgONV9cPxJ0m6FEOO1DcC3wRuSnJk7cfXRt4laU4zT5RV1e+A/A+2\nSFoA31EmNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj\n1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPU\nUjNGLTVj1FIzg6NOsiXJi0meHnOQpEuzniP1fcDxsYZIWoxBUSfZDdwCPDruHEmXauiR+kfA/cD7\nF7pDkv1JVpKsnOHUQsZJWr+ZUSe5FfhLVb1wsftV1YGqWq6q5a1sW9hASesz5Eh9I/D1JK8BjwM3\nJfn5qKskzW1m1FX1YFXtrqqrgduB31bVnaMvkzQX/51aamZpPXeuqueB50dZImkhPFJLzRi11IxR\nS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFL\nzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvN\nDIo6yeVJDib5Q5LjSb4w9jBJ81kaeL8fA7+sqm8kuQzYMeImSZdgZtRJPgZ8CfgWQFWdBk6PO0vS\nvIY8/d4DnAR+luTFJI8m2XnunZLsT7KSZOUMpxY+VNIwQ6JeAj4HPFJV1wHvAg+ce6eqOlBVy1W1\nvJVtC54paaghUZ8ATlTVobXLB1mNXNIGNDPqqnoHeCPJNWtX7QOOjbpK0tyGnv2+B3hs7cz3q8Bd\n402SdCkGRV1VR4DlkbdIWgDfUSY1Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj\n1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0P/Ly2t\n07NvHZl6wrqMtffLV147yuPqwjxSS80YtdSMUUvNGLXUjFFLzRi11IxRS80MijrJd5McTfJykl8k\n2T72MEnzmRl1kl3AvcByVX0G2ALcPvYwSfMZ+vR7CfhwkiVgB/DWeJMkXYqZUVfVm8APgNeBt4G/\nV9Wvzr1fkv1JVpKsnOHU4pdKGmTI0+8rgNuAPcCVwM4kd557v6o6UFXLVbW8lW2LXyppkCFPv28G\n/lRVJ6vqDPAk8MVxZ0ma15CoXwduSLIjSYB9wPFxZ0ma15DX1IeAg8Bh4KW1X3Ng5F2S5jTo+6mr\n6iHgoZG3SFoA31EmNWPUUjNGLTVj1FIzRi0146eJjsRP0dRUPFJLzRi11IxRS80YtdSMUUvNGLXU\njFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS82kqhb/\noMlJ4M8D7vpx4K8LHzCezbR3M22FzbV3I2z9ZFV94oNuGCXqoZKsVNXyZAPWaTPt3UxbYXPt3ehb\nffotNWPUUjNTR73Z/vP6zbR3M22FzbV3Q2+d9DW1pMWb+kgtacGMWmpmsqiTfCXJH5O8kuSBqXbM\nkuSqJM8lOZbkaJL7pt40RJItSV5M8vTUWy4myeVJDib5Q5LjSb4w9aaLSfLdta+Dl5P8Isn2qTed\na5Kok2wBHga+CuwF7kiyd4otA7wHfK+q9gI3AN/ewFvPdh9wfOoRA/wY+GVVfRr4LBt4c5JdwL3A\nclV9BtgC3D7tqvNNdaS+Hnilql6tqtPA48BtE225qKp6u6oOr/38n6x+0e2adtXFJdkN3AI8OvWW\ni0nyMeBLwE8Aqup0Vf1t2lUzLQEfTrIE7ADemnjPeaaKehfwxlmXT7DBQwFIcjVwHXBo2iUz/Qi4\nH3h/6iEz7AFOAj9be6nwaJKdU4+6kKp6E/gB8DrwNvD3qvrVtKvO54mygZJ8BHgC+E5V/WPqPReS\n5FbgL1X1wtRbBlgCPgc8UlXXAe8CG/n8yhWsPqPcA1wJ7Exy57SrzjdV1G8CV511effadRtSkq2s\nBv1YVT059Z4ZbgS+nuQ1Vl/W3JTk59NOuqATwImq+s8zn4OsRr5R3Qz8qapOVtUZ4EngixNvOs9U\nUf8e+FSSPUkuY/Vkw1MTbbmoJGH1Nd/xqvrh1HtmqaoHq2p3VV3N6p/rb6tqwx1NAKrqHeCNJNes\nXbUPODbhpFleB25IsmPt62IfG/DE3tIUv2lVvZfkbuBZVs8g/rSqjk6xZYAbgW8CLyU5snbd96vq\nmQk3dXIP8NjaX+6vAndNvOeCqupQkoPAYVb/VeRFNuBbRn2bqNSMJ8qkZoxaasaopWaMWmrGqKVm\njFpqxqilZv4NgcIHD1OJ/kQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4STMYtvK0rH",
        "colab_type": "text"
      },
      "source": [
        "Notice that our algorithm does work when determining an exact string from our orignal matrix. Now we will take out four points from the string and see if our algorithm is able to determine that it is the letter I. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a4zPnDt-Ms4",
        "colab_type": "code",
        "outputId": "ecbfb468-dd44-49ff-fcca-be854c819ce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "#Here is where we manual take out 4 points from the letter I\n",
        "New_I = [8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "New_I = np.asarray(New_I)\n",
        "plt.imshow(New_I[1:].reshape(10,10))\n",
        "print(New_I)\n",
        "feature_vect = get_feature_vec(New_I)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[8])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "feature vect of input list is: [96. 95. 96. 96. 96.  0.]\n",
            "standard feature vect of I is: [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 8 th letter \n",
            " the closest distance is: 2.23606797749979 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAJd0lEQVR4nO3dzYtdBx2H8edrJk2aKLagmybFZiFK\nEGxl0NpCF02hvhS7cVGhgm6ysW8ilNZN/wGRuhAhVN1Y2kXaRSnFKrYu3IRO04AmUShV2/SFxoVW\nCiYp/bmYEWLS5J65uadn5sfzgUDuS26+hHly7j25c5OqQlIfH5l6gKTFMmqpGaOWmjFqqRmjlppZ\nGuNBL8u22s7OMR5aEvAf3uV0ncoH3TZK1NvZyZeyb4yHlgQcqt9d8DaffkvNGLXUjFFLzRi11IxR\nS80YtdTMoKiTfCXJX5K8nOSBsUdJmt/MqJNsAX4KfBXYC3wryd6xh0maz5Aj9ReBl6vqlao6DTwO\n3D7uLEnzGhL1LuC1sy6fWLvu/yTZn2QlycoZTi1qn6R1WtiJsqo6UFXLVbW8lW2LelhJ6zQk6teB\nq8+6vHvtOkkb0JCoXwA+nWRPksuAO4Cnxp0laV4zv0urqt5LchfwLLAF+EVVHR19maS5DPrWy6p6\nBnhm5C2SFsB3lEnNGLXUjFFLzRi11IxRS82M8sGD2nyefePIKI9761XXjvK4ujCP1FIzRi01Y9RS\nM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIz\nRi01Y9RSM36a6Cbjp35qFo/UUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjMzo05ydZLnkxxLcjTJvR/G\nMEnzGfLmk/eAH1TV4SQfA15M8tuqOjbyNklzmHmkrqo3q+rw2s//DRwHdo09TNJ81vU20STXANcB\nhz7gtv3AfoDt7FjANEnzGHyiLMlHgSeA+6rqnXNvr6oDVbVcVctb2bbIjZLWYVDUSbayGvSjVfXk\nuJMkXYohZ78D/Bw4XlU/Hn+SpEsx5Eh9I/Bt4OYkR9Z+fG3kXZLmNPNEWVX9AciHsEXSAviOMqkZ\no5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmj\nlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOW\nmhkcdZItSV5K8vSYgyRdmvUcqe8Fjo81RNJiDIo6yW7g68Aj486RdKmGHqkfBu4H3r/QHZLsT7KS\nZOUMpxYyTtL6zYw6yW3A21X14sXuV1UHqmq5qpa3sm1hAyWtz5Aj9Y3AN5L8DXgcuDnJr0ZdJWlu\nM6OuqgerandVXQPcATxXVXeOvkzSXPx3aqmZpfXcuap+D/x+lCWSFsIjtdSMUUvNGLXUjFFLzRi1\n1IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXU\njFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11MygqJNckeRg\nkj8nOZ7ky2MPkzSfpYH3+wnw66r6ZpLLgB0jbpJ0CWZGneTjwE3AdwCq6jRwetxZkuY15On3HuAk\n8MskLyV5JMnOc++UZH+SlSQrZzi18KGShhkS9RLwBeBnVXUd8C7wwLl3qqoDVbVcVctb2bbgmZKG\nGhL1CeBEVR1au3yQ1cglbUAzo66qt4DXknxm7ap9wLFRV0ma29Cz33cDj66d+X4F+O54kyRdikFR\nV9URYHnkLZIWwHeUSc0YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80Y\ntdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdTM0P9LSxvErVddO8rj\nPvvGkVEed6y9ujCP1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzg6JO8v0kR5P8KcljSbaPPUzSfGZG\nnWQXcA+wXFWfA7YAd4w9TNJ8hj79XgIuT7IE7ADeGG+SpEsxM+qqeh34EfAq8Cbwr6r6zbn3S7I/\nyUqSlTOcWvxSSYMMefp9JXA7sAe4CtiZ5M5z71dVB6pquaqWt7Jt8UslDTLk6fctwF+r6mRVnQGe\nBG4Yd5akeQ2J+lXg+iQ7kgTYBxwfd5akeQ15TX0IOAgcBv649msOjLxL0pwGfT91VT0EPDTyFkkL\n4DvKpGaMWmrGqKVmjFpqxqilZvw0UQF+6mcnHqmlZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasao\npWaMWmrGqKVmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasaopWZSVYt/0OQk8PcBd/0E\n8I+FDxjPZtq7mbbC5tq7EbZ+qqo++UE3jBL1UElWqmp5sgHrtJn2bqatsLn2bvStPv2WmjFqqZmp\no95s/3n9Ztq7mbbC5tq7obdO+ppa0uJNfaSWtGBGLTUzWdRJvpLkL0leTvLAVDtmSXJ1kueTHEty\nNMm9U28aIsmWJC8leXrqLReT5IokB5P8OcnxJF+eetPFJPn+2tfBn5I8lmT71JvONUnUSbYAPwW+\nCuwFvpVk7xRbBngP+EFV7QWuB763gbee7V7g+NQjBvgJ8Ouq+izweTbw5iS7gHuA5ar6HLAFuGPa\nVeeb6kj9ReDlqnqlqk4DjwO3T7Tloqrqzao6vPbzf7P6Rbdr2lUXl2Q38HXgkam3XEySjwM3AT8H\nqKrTVfXPaVfNtARcnmQJ2AG8MfGe80wV9S7gtbMun2CDhwKQ5BrgOuDQtEtmehi4H3h/6iEz7AFO\nAr9ce6nwSJKdU4+6kKp6HfgR8CrwJvCvqvrNtKvO54mygZJ8FHgCuK+q3pl6z4UkuQ14u6penHrL\nAEvAF4CfVdV1wLvARj6/ciWrzyj3AFcBO5PcOe2q800V9evA1Wdd3r123YaUZCurQT9aVU9OvWeG\nG4FvJPkbqy9rbk7yq2knXdAJ4ERV/e+Zz0FWI9+obgH+WlUnq+oM8CRww8SbzjNV1C8An06yJ8ll\nrJ5seGqiLReVJKy+5jteVT+ees8sVfVgVe2uqmtY/XN9rqo23NEEoKreAl5L8pm1q/YBxyacNMur\nwPVJdqx9XexjA57YW5riN62q95LcBTzL6hnEX1TV0Sm2DHAj8G3gj0mOrF33w6p6ZsJNndwNPLr2\nl/srwHcn3nNBVXUoyUHgMKv/KvISG/Ato75NVGrGE2VSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM/8F\nBnQEoOIKiyUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itfkeOHCavBa",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the picture above, we have removed some of the points in I. However, this did not affect our algorithm as we are still getting that the best fitted letter is the 8th letter (I). Now we will randomly remove points to make sure that our algrothim is as robust as we believe it is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6ZYIH6Zlgc",
        "colab_type": "code",
        "outputId": "2f5ef1d9-ec32-4d62-f3f9-09820e0b1d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "source": [
        "def takeoff_pts(letter_one_line, num_of_pts): #input the 010101001.. sequece of a letter, and the number of points you want to take of from this letter, say 2, or 5 etc.\n",
        "    one_list = []\n",
        "    l = len(letter_one_line)\n",
        "    for i in range(l):\n",
        "        if letter_one_line[i]==1:\n",
        "            one_list.append(i)\n",
        "\n",
        "    zero_posi = np.random.choice(one_list, size= num_of_pts)\n",
        "\n",
        "    minus_one = np.zeros(101)\n",
        "    \n",
        "    for i in zero_posi:\n",
        "        minus_one[i] = -1\n",
        "    \n",
        "    Noise_letter0 = letter_one_line + minus_one\n",
        "\n",
        "    return Noise_letter0\n",
        "\n",
        "letter_I = letters[8,:]\n",
        "\n",
        "\n",
        "Noise_I = takeoff_pts(letter_I, 3)\n",
        "\n",
        "Noise_feature0 = get_feature_vec(Noise_I)\n",
        "print('Feature vector of Noise I:\\n',Noise_feature0,'\\n','Original feature vector of I:\\n',vec_mtx[8])\n",
        "\n",
        "Noise_I = np.asarray(Noise_I)\n",
        "plt.imshow(Noise_I[1:].reshape(10,10))\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(Noise_feature0 - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature vector of Noise I:\n",
            " [193. 192. 189. 188. 190.   0.] \n",
            " Original feature vector of I:\n",
            " [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 4 th letter \n",
            " the closest distance is: 201.09584121438755 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAJg0lEQVR4nO3dzYtdBx2H8edrJk1MFC3opkmxWYgS\nBG0Zal/ARVPwFbtxUaGCbrKxtUpBqhv/gVLsohRCXzYt7SLtokgxii8LN8FpGtAkCqVqm6bFuLBK\nwSSlPxczQkya3DM39/TM/Hg+EMh9yc2XME/OvSd3blJVSOrjA1MPkLRYRi01Y9RSM0YtNWPUUjNL\nYzzoVdlW29k5xkNLAv7D25ytM3mv20aJejs7+Xz2jfHQkoDD9atL3ubTb6kZo5aaMWqpGaOWmjFq\nqRmjlpoZFHWSLyX5c5KXk9w/9ihJ85sZdZItwMPAl4G9wDeT7B17mKT5DDlS3wi8XFWvVNVZ4Bng\njnFnSZrXkKh3Aa+dd/nk2nX/J8n+JCtJVs5xZlH7JK3Twk6UVdWBqlququWtbFvUw0papyFRvw5c\ne97l3WvXSdqAhkT9e+CTSfYkuQq4E3h+3FmS5jXzu7Sq6p0kdwOHgC3A41V1bPRlkuYy6Fsvq+oF\n4IWRt0haAN9RJjVj1FIzRi01Y9RSM0YtNTPKBw8KDp06OvWEdfniNZ+beoIWxCO11IxRS80YtdSM\nUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxR\nS80YtdSMnyY6Ej+dU1PxSC01Y9RSM0YtNWPUUjNGLTVj1FIzRi01MzPqJNcm+U2S40mOJbn3/Rgm\naT5D3nzyDnBfVR1J8mHgxSS/rKrjI2+TNIeZR+qqeqOqjqz9/N/ACWDX2MMkzWddbxNNch1wPXD4\nPW7bD+wH2M6OBUyTNI/BJ8qSfAh4Fvh+Vf3rwtur6kBVLVfV8la2LXKjpHUYFHWSrawG/VRVPTfu\nJElXYsjZ7wCPASeq6sHxJ0m6EkOO1LcC3wJuS3J07cdXRt4laU4zT5RV1e+AvA9bJC2A7yiTmjFq\nqRmjlpoxaqkZP3hwkzl06ugoj+sHJfbhkVpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasao\npWaMWmrGqKVmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqil\nZoxaasaopWaMWmpmcNRJtiR5KcnPxhwk6cqs50h9L3BirCGSFmNQ1El2A18FHh13jqQrNfRI/VPg\nh8C7l7pDkv1JVpKsnOPMQsZJWr+ZUSf5GvD3qnrxcverqgNVtVxVy1vZtrCBktZnyJH6VuDrSf4K\nPAPcluTJUVdJmtvMqKvqR1W1u6quA+4Efl1Vd42+TNJc/HdqqZml9dy5qn4L/HaUJZIWwiO11IxR\nS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFL\nzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvN\nGLXUzKCok3w0ycEkf0pyIsnNYw+TNJ+lgfd7CPh5VX0jyVXAjhE3SboCM6NO8hHgC8C3AarqLHB2\n3FmS5jXk6fce4DTwRJKXkjyaZOeFd0qyP8lKkpVznFn4UEnDDIl6CbgBeKSqrgfeBu6/8E5VdaCq\nlqtqeSvbFjxT0lBDoj4JnKyqw2uXD7IauaQNaGbUVfUm8FqST61dtQ84PuoqSXMbevb7HuCptTPf\nrwDfGW+SpCsxKOqqOgosj7xF0gL4jjKpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpox\naqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmhn6\nf2lpnQ6dOjr1hHUZa+8Xr/ncKI+rS/NILTVj1FIzRi01Y9RSM0YtNWPUUjNGLTUzKOokP0hyLMkf\nkzydZPvYwyTNZ2bUSXYB3wOWq+ozwBbgzrGHSZrP0KffS8AHkywBO4BT402SdCVmRl1VrwMPAK8C\nbwBvVdUvLrxfkv1JVpKsnOPM4pdKGmTI0++rgTuAPcA1wM4kd114v6o6UFXLVbW8lW2LXyppkCFP\nv28H/lJVp6vqHPAccMu4syTNa0jUrwI3JdmRJMA+4MS4syTNa8hr6sPAQeAI8Ie1X3Ng5F2S5jTo\n+6mr6ifAT0beImkBfEeZ1IxRS80YtdSMUUvNGLXUjJ8mOhI/RVNT8UgtNWPUUjNGLTVj1FIzRi01\nY9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTWT\nqlr8gyangb8NuOvHgH8sfMB4NtPezbQVNtfejbD1E1X18fe6YZSoh0qyUlXLkw1Yp820dzNthc21\nd6Nv9em31IxRS81MHfVm+8/rN9PezbQVNtfeDb110tfUkhZv6iO1pAUzaqmZyaJO8qUkf07ycpL7\np9oxS5Jrk/wmyfEkx5LcO/WmIZJsSfJSkp9NveVyknw0ycEkf0pyIsnNU2+6nCQ/WPs6+GOSp5Ns\nn3rThSaJOskW4GHgy8Be4JtJ9k6xZYB3gPuqai9wE/DdDbz1fPcCJ6YeMcBDwM+r6tPAZ9nAm5Ps\nAr4HLFfVZ4AtwJ3TrrrYVEfqG4GXq+qVqjoLPAPcMdGWy6qqN6rqyNrP/83qF92uaVddXpLdwFeB\nR6fecjlJPgJ8AXgMoKrOVtU/p1010xLwwSRLwA7g1MR7LjJV1LuA1867fJINHgpAkuuA64HD0y6Z\n6afAD4F3px4ywx7gNPDE2kuFR5PsnHrUpVTV68ADwKvAG8BbVfWLaVddzBNlAyX5EPAs8P2q+tfU\ney4lydeAv1fVi1NvGWAJuAF4pKquB94GNvL5latZfUa5B7gG2JnkrmlXXWyqqF8Hrj3v8u616zak\nJFtZDfqpqnpu6j0z3Ap8PclfWX1Zc1uSJ6eddEkngZNV9b9nPgdZjXyjuh34S1WdrqpzwHPALRNv\nushUUf8e+GSSPUmuYvVkw/MTbbmsJGH1Nd+Jqnpw6j2zVNWPqmp3VV3H6p/rr6tqwx1NAKrqTeC1\nJJ9au2ofcHzCSbO8CtyUZMfa18U+NuCJvaUpftOqeifJ3cAhVs8gPl5Vx6bYMsCtwLeAPyQ5unbd\nj6vqhQk3dXIP8NTaX+6vAN+ZeM8lVdXhJAeBI6z+q8hLbMC3jPo2UakZT5RJzRi11IxRS80YtdSM\nUUvNGLXUjFFLzfwXh1AHEbNUDO4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2MLdv9-avBW",
        "colab_type": "text"
      },
      "source": [
        "Now we are not getting an accurate best fitted letter. To see exactly how the result is affected by the position of noise point, we will again manually remove a few points and examine its effect. Lets take off the upper bar of I."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnYz2DRFfIvj",
        "colab_type": "code",
        "outputId": "ae7af35d-4fc2-4fce-b00e-fe962e69e197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "New_I = [8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "New_I = np.asarray(New_I)\n",
        "plt.imshow(New_I[1:].reshape(10,10))\n",
        "print(New_I)\n",
        "feature_vect = get_feature_vec(New_I)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[8])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "feature vect of input list is: [96. 96. 95. 96. 95.  0.]\n",
            "standard feature vect of I is: [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 8 th letter \n",
            " the closest distance is: 2.0 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOdMHKgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp_p6f68fmuH",
        "colab_type": "text"
      },
      "source": [
        "Again, the best fitted letter is I so let's remove a center point and see what happens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQPU6FFQfmIe",
        "colab_type": "code",
        "outputId": "4dc3d94e-1174-4556-8814-c66e49ae9fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "#Here is where we make up a test list. We will be using the letter I (the 8th letter) in 10X10 grid.\n",
        "New_I = [8,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "New_I = np.asarray(New_I)\n",
        "plt.imshow(New_I[1:].reshape(10,10))\n",
        "print(New_I)\n",
        "feature_vect = get_feature_vec(New_I)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[8])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "feature vect of input list is: [191. 192. 190. 190. 190.   0.]\n",
            "standard feature vect of I is: [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 4 th letter \n",
            " the closest distance is: 201.45604322958934 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqti6epOQm7u",
        "colab_type": "text"
      },
      "source": [
        "From above result, we see if we took a point at center of I off, it changes drastically. The feature vector of new I has a norm greater than most of the vectors in feature matrix, so the new result is letter E, since it has the greatest norm in our feature map, therefore closer to our new letter I."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9C3EcnEZAjoi"
      },
      "source": [
        "### Advantages of the classification system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hyFz1nKY_kb",
        "colab_type": "text"
      },
      "source": [
        "The main accomplishment of our classification system is that we have successfully differentiated between letters. We know that this is true since the minimum difference between each letter pair is greater than zero. Our algorithm allows us to bring our scanning and h1 methods into other systems for classification. Some of these systems could be Script letters, the Greek alphabet or the Arabic alphabet. As long as they are a collection of 10x10 images our algorithm would be able to classify it. Another benefit of our algorithm is that unlike other ML sets it does not need to go through training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGTTvSrtavBb",
        "colab_type": "text"
      },
      "source": [
        "### Criticism of the classification system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3un4zHGavBc",
        "colab_type": "text"
      },
      "source": [
        "Though our classification system does not always classify letters correctly we do have complete control over the algorithm. We observe that the results are not robust against eliminating random points from letters. Especiallly when the letter is broken into different parts. Our system tends to produce a feature vector with a norm larger than any of those in feature matrix, therefore it mis-recognizes the letter. Given more time to research, we may be able to find a way to filter out these feature vectors with exceedingly large norm, thereby improving the system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59bDQg9aeEY9",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "In conclusion, we take a sequence of zeros and ones representing a letter, then scan them using multi-dimensional and multi-directional algorithms, to compose its feature vector and we compare it with other sequences . Image vectorization is a good way to deal with image recognition in topological data analysis, machine learning, deep learning and even in reinforcement learning. We believe that if we had any collection of 10x10 images, our algorithm would be able to recongize the image. If the final result of our algorithm is not ideal for the recognition rate of new handwritten letters, the reasons may be as follows: First, the image feature extraction is too simple, the edges of the image are blank, and the center positions of the letters in the image may not all correspond; Then, the sample size is small, and each character has only one 10x10 array sample. The real 10x10 collection of images requires massive data. Of course, we can also try to improve the classification recognition rate with other learning algorithms, such as KNN algorithm, SVM algorithm, logistic regression, neural network, etc."
      ]
    }
  ]
}