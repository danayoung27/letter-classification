{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lettter Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wnF4KOGyavBC",
        "udisgjcUjvjn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKnRXs-iavBB",
        "colab_type": "text"
      },
      "source": [
        "# Classification of the Latin alphabet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnF4KOGyavBC",
        "colab_type": "text"
      },
      "source": [
        "### Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC5moJH_avBD",
        "colab_type": "text"
      },
      "source": [
        "For this project we were asked to classify a letter given a string of 100 zeros and ones using persistent homology. We were told that the algorithm should be able to compute a vector of features that we can use to compare to the other features of the Latin alphabet. The features that my group decided to use were probing from the upper left to lower right, upper right to lower left, bottom right to the upper left, scanning from left to right, scanning from right to left, and homology 1. Below are pictures of each of the scans:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxldmFcgnOMM",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=16ItJF8t6IQ_0TVW4zZ7Tl324fbCQt0hH)\n",
        "![alt text](https://drive.google.com/uc?id=1QIxpbakPRpG65r9n37-eKGqIiJj81mdc)\n",
        "![alt text](https://drive.google.com/uc?id=1fnV8PKe2kDIGk2-qcI9JsReK4XYQme1d)\n",
        "![alt text](https://drive.google.com/uc?id=1fnWEwOQSX5uls76EgrM6JgzyxaM5VJtO)\n",
        "![alt text](https://drive.google.com/uc?id=1HzEaNr_AX8HOLy3AYagcq_8XdmHy8Nsi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyDhYU6_avBE",
        "colab_type": "text"
      },
      "source": [
        "### Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zbr_2rPqURC",
        "colab_type": "text"
      },
      "source": [
        "In this work, we are going to scan all 26 letters of the Latin alphabet in different ways by using persistent homology. Persistent homology is a method for computing change of topological features of a space. With Lower Star Image Filtrations, we were able to find the life and death pair for each single letter in 0-dimensional homology. Lower Star Image Filrations allows us to express our local minimums as birth times and our saddle points as death times. This is useful in our project since it gives us a way to summarize the critical points of our image. Next, we convert all the pairs to feature vector for each letter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "udisgjcUjvjn"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGFTkX3Yj53B",
        "colab_type": "code",
        "outputId": "cfccfe54-5f2b-4a93-eea0-c387931424c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "pip install scikit-tda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-tda\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/67/14676df9d3c8fae3324f6a73e60e9b705c1dfea8131e275a313ab90bd09d/scikit_tda-0.0.3-py3-none-any.whl\n",
            "Collecting kmapper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/3f/a1290b9425a7e0ff4ae51a6e6ff68e50ad793b3460f435c2ec81c0383751/kmapper-1.2.0-py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.6MB/s \n",
            "\u001b[?25hCollecting ripser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/17/d5e898161eb8e3b1084419bfc04adbe87d15075daa8e236df72dbe75167e/ripser-0.4.1.tar.gz (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hCollecting tadasets\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/06/d1b9edccfcd071b245b0d1ab4b22eb2ff7aaeaa6d015db58d701d9782122/tadasets-0.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (1.17.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (4.3.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (0.3.10)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (0.29.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (0.21.3)\n",
            "Collecting persim\n",
            "  Downloading https://files.pythonhosted.org/packages/71/4a/ac537e6743337b00a8b3ff8b0d967827d3b5cfd9afd3a0bd117f5809d4d2/persim-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (1.3.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from scikit-tda) (0.40.1)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from kmapper->scikit-tda) (2.10.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->scikit-tda) (0.46)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-tda) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-tda) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-tda) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-tda) (2.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->scikit-tda) (0.14.0)\n",
            "Collecting hopcroftkarp\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/56/7b03eba3c43008c490c9d52e69ea5334b65955f66836eb4f1962f3b0d421/hopcroftkarp-1.2.5.tar.gz\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->scikit-tda) (0.30.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2->kmapper->scikit-tda) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->scikit-tda) (41.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->scikit-tda) (1.12.0)\n",
            "Building wheels for collected packages: ripser, hopcroftkarp\n",
            "  Building wheel for ripser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ripser: filename=ripser-0.4.1-cp36-cp36m-linux_x86_64.whl size=423321 sha256=463797dadab19583759fb388bc180f374a6dcf31c4418117f16c634ca3547a23\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/12/da/56d55c3af62ebd5e0684d521f3e58c1a85ac312502c9e2d47d\n",
            "  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopcroftkarp: filename=hopcroftkarp-1.2.5-py2.py3-none-any.whl size=18091 sha256=8d7b9046fe4bd74e18e72b373af0d3894e39ea2f0dffca5299c22ba333135e62\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/e1/c9/1993c7f7f114b7d3fb2d3e895e02157a7ebf554861e9e54e01\n",
            "Successfully built ripser hopcroftkarp\n",
            "Installing collected packages: kmapper, hopcroftkarp, persim, ripser, tadasets, scikit-tda\n",
            "Successfully installed hopcroftkarp-1.2.5 kmapper-1.2.0 persim-0.1.1 ripser-0.4.1 scikit-tda-0.0.3 tadasets-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtHm743sj_Bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import Series, DataFrame\n",
        "from numpy import genfromtxt\n",
        "from ripser import ripser,lower_star_img\n",
        "from persim import plot_diagrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfH68c7okFlU",
        "colab_type": "code",
        "outputId": "aad12dc6-20e5-462b-f273-dcfabfe6574b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded= files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f7b52bd6-7a32-42a2-b6e2-2a52623eeba8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f7b52bd6-7a32-42a2-b6e2-2a52623eeba8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving letters.csv to letters.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2SGNSaavBF",
        "colab_type": "text"
      },
      "source": [
        "### First part of the algrorithm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUJXkum7avBG",
        "colab_type": "text"
      },
      "source": [
        "For all five scans, we use the lower_star_img package from ripser, to obtain the coordinates of each of the 0-order life-death pairs. At the additional last test, we run 1st order homology test for each letters and also obtain their coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OKMR1ZbM0VS",
        "colab_type": "code",
        "outputId": "432957ee-f3ec-4349-9006-e6c0a925af2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "letters = genfromtxt('letters.csv', delimiter =',')\n",
        "#the main block: run 6 scans for each of the 26 letters, store results in a dictionary.\n",
        "i=0\n",
        "i_to_result = dict()\n",
        "while i<=25:\n",
        "    letter_one_line = letters[i,:]\n",
        "#initialize matrix of size 10x10 with all values 100\n",
        "    letter= np.full((10,10),100)\n",
        "   #test 1 probing upper left\n",
        "    for k in range(1,100):\n",
        "        if letter_one_line[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max((k-1)%10, int((k-1)/10))\n",
        "    PULscan = lower_star_img(letter)\n",
        "    #test 2 left to right\n",
        "    for j in range(1,100):\n",
        "        if letter_one_line[j]==1.0:\n",
        "            row=int((j-1)/10)\n",
        "            column= (j-1)%10\n",
        "            letter[row,column]= j%10\n",
        "    LRscan = lower_star_img(letter)\n",
        "    #test 3 right to left\n",
        "    for k in range(1,100):\n",
        "        if letter_one_line[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= 10 - (k-1)%10\n",
        "    RLscan = lower_star_img(letter)\n",
        "    #test 4 probing from bottom right\n",
        "    for k in range(1,100):\n",
        "        if letter_one_line[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max(9-(k-1)%10,9-int((k-1)/10))  \n",
        "    PLBscan = lower_star_img(letter)\n",
        "    #test 5 probing from upper right to lower left \n",
        "    for k in range(1,100):       \n",
        "        if letter_one_line[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max(9-(k-1)%10,int((k-1)/10))  \n",
        "    PURscan = lower_star_img(letter) \n",
        "    #test 6\n",
        "    letter_ones_line = letters[i,1:]\n",
        "    newletter = letter_ones_line.reshape(10,10)\n",
        "    coordinates = np.argwhere(newletter == 1)\n",
        "    dgms = ripser(coordinates)['dgms']\n",
        "    h1_test = dgms[1]\n",
        "    \n",
        "    i_to_result[i] = (PULscan, LRscan, RLscan, PLBscan, PURscan, h1_test)\n",
        "    i+=1\n",
        "\n",
        "#change all infinity in life-death pair to 100.    \n",
        "l=0\n",
        "while l <=25:\n",
        "    for j in range(6):\n",
        "        len_j = len(i_to_result[l][j])\n",
        "        for k in range(len_j):\n",
        "            if str(i_to_result[l][j][k][1])=='inf':\n",
        "                i_to_result[l][j][k][1]=100\n",
        "    l+=1\n",
        "#calculate feature vector for each letter, store them in a matrix. \n",
        "#every feature vector has 5 components, each component stands for the sum of \"lifespan\" of the life-death pairs in one scan.\n",
        "l=0\n",
        "vec_mtx = np.zeros((26,6))\n",
        "while l < 26:\n",
        "    for j in range(6):\n",
        "        len_j = len(i_to_result[l][j])\n",
        "        sum_j = 0\n",
        "        for k in range(len_j):\n",
        "            sum_j = sum_j+ (i_to_result[l][j][k][1]-i_to_result[l][j][k][0])\n",
        "            vec_mtx[l][j] = sum_j\n",
        "    l+=1\n",
        "print('feature matrix:\\n',vec_mtx,'\\n')\n",
        "\n",
        "#calculate the pairwise distance between each two letter, store them in a list.\n",
        "all_dis = []\n",
        "for q in range(26):\n",
        "    for p in range(q):\n",
        "            dis = np.linalg.norm(vec_mtx[q]-vec_mtx[p])\n",
        "            all_dis.append((dis,p,q))\n",
        "\n",
        "#sort all the distance from low to high, find the minimum distance, also find which letter pair has the minimum distance.             \n",
        "def getkey(item):\n",
        "    return item[0]\n",
        "\n",
        "sorted_dis = sorted(all_dis, key=getkey)\n",
        "print('how many items in this sorted distance list:',len(sorted_dis),'\\n','sorted distance:',sorted_dis,'\\n')\n",
        "\n",
        "print('min distance is between',sorted_dis[0][1],'th letter and',sorted_dis[0][2],'th letter. min distance is',sorted_dis[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature matrix:\n",
            " [[ 97.          98.          97.          98.          97.\n",
            "    3.48528123]\n",
            " [ 98.          97.          98.          98.          97.\n",
            "    6.23334539]\n",
            " [ 98.          98.         102.          98.          99.\n",
            "    0.82842708]\n",
            " [ 98.          97.          98.          98.          98.\n",
            "    5.24264061]\n",
            " [ 98.          97.         107.         100.         101.\n",
            "    0.73304415]\n",
            " [ 98.          97.         100.          95.         100.\n",
            "    0.3245554 ]\n",
            " [ 98.          98.         100.          97.          99.\n",
            "    2.42366862]\n",
            " [ 98.          97.          96.          97.          97.\n",
            "    0.12310553]\n",
            " [ 97.          97.          96.          96.          96.\n",
            "    0.        ]\n",
            " [ 97.          98.          95.          96.          96.\n",
            "    0.        ]\n",
            " [ 98.          97.          99.          97.          98.\n",
            "    0.82842708]\n",
            " [ 98.          97.          96.          97.          93.\n",
            "    0.        ]\n",
            " [ 99.          98.          97.         100.          98.\n",
            "    2.12899017]\n",
            " [ 99.          98.          96.          97.          97.\n",
            "    0.82842708]\n",
            " [ 98.          98.          97.          99.          99.\n",
            "    4.41673815]\n",
            " [ 98.          97.          97.          96.          98.\n",
            "    2.57649124]\n",
            " [ 99.          99.          97.          99.         100.\n",
            "    4.41421354]\n",
            " [ 99.          97.          98.          97.          97.\n",
            "    3.24264061]\n",
            " [ 98.         102.         102.          98.         100.\n",
            "    0.57076645]\n",
            " [ 99.          98.          97.          95.          98.\n",
            "    0.        ]\n",
            " [ 99.          97.          97.          98.          99.\n",
            "    0.29705858]\n",
            " [100.          98.          97.          96.          98.\n",
            "    0.41421354]\n",
            " [100.          98.          98.          99.         101.\n",
            "    2.60112619]\n",
            " [ 98.          98.          98.          97.          97.\n",
            "    2.3071357 ]\n",
            " [ 98.          97.          96.          96.          97.\n",
            "    0.47213602]\n",
            " [101.         102.         101.          99.          98.\n",
            "    0.82842708]] \n",
            "\n",
            "how many items in this sorted distance list: 325 \n",
            " sorted distance: [(1.0591611238255987, 7, 24), (1.4076561905961278, 1, 3), (1.4142135623730951, 8, 9), (1.4736257514047435, 19, 21), (1.4909434670721213, 8, 24), (1.5803412572654365, 7, 13), (1.6956324610199927, 17, 23), (1.7320526474901206, 14, 16), (1.7364201595594493, 7, 8), (1.7683165204106095, 13, 24), (2.0180565910073907, 15, 23), (2.042442864611687, 13, 21), (2.0549726085777977, 9, 24), (2.094761772796275, 0, 23), (2.1080215828055, 15, 17), (2.163819511130015, 3, 14), (2.239454167989259, 7, 9), (2.2774062433337536, 10, 23), (2.2853692091224644, 19, 24), (2.3547389647468817, 6, 10), (2.535411381850512, 15, 24), (2.5857864221185745, 13, 19), (2.6206129054684464, 0, 14), (2.6457513110645907, 3, 17), (2.646385273156172, 21, 24), (2.6483438756482296, 20, 21), (2.6514636836676537, 7, 20), (2.656854242861205, 0, 17), (2.6623884032520984, 0, 3), (2.6807795875781286, 13, 23), (2.698583420324775, 10, 20), (2.698583420324775, 13, 20), (2.6994973083127967, 16, 22), (2.746779130563143, 2, 6), (2.772416170204751, 8, 13), (2.772416170204751, 9, 13), (2.7974808757458254, 0, 15), (2.8308267205643594, 6, 23), (2.8311049027763815, 7, 19), (2.8382614940645636, 10, 15), (2.8382614940645636, 13, 15), (2.843368402927536, 7, 21), (2.8694582792174432, 12, 14), (2.9712669028349907, 10, 17), (3.003181886857116, 7, 15), (3.04202016252468, 5, 10), (3.058753562192961, 12, 20), (3.0605920713981494, 23, 24), (3.104562302921846, 15, 19), (3.1105377100275295, 15, 21), (3.1256979710496107, 7, 23), (3.167120475987734, 20, 24), (3.1892903372385515, 10, 21), (3.209370943505916, 1, 14), (3.2258760570274823, 5, 6), (3.2399812483122954, 7, 10), (3.248362145664116, 0, 1), (3.253176035079121, 21, 23), (3.266606055667772, 6, 17), (3.268989357708706, 10, 19), (3.290657534275276, 13, 17), (3.292343455308958, 0, 12), (3.3082193171478025, 1, 17), (3.3166247903554, 8, 11), (3.3166247903554, 8, 19), (3.3299014702122336, 19, 20), (3.335707318749216, 10, 24), (3.3460145286381366, 15, 20), (3.3499620635411715, 12, 16), (3.408399785263611, 3, 23), (3.418521818098106, 3, 16), (3.4641016151377544, 2, 10), (3.4641016151377544, 10, 13), (3.4674709445448895, 6, 15), (3.4686792629312486, 12, 23), (3.4699293975265806, 20, 23), (3.479705807229299, 3, 15), (3.4961282044577597, 12, 22), (3.5066289772175394, 14, 22), (3.5103952973500068, 19, 23), (3.518309967686826, 14, 17), (3.555039675267126, 8, 15), (3.5625081570823873, 12, 13), (3.6055512374908316, 17, 21), (3.605551275463989, 9, 11), (3.605551275463989, 9, 19), (3.613706405978494, 0, 13), (3.638710940033236, 12, 17), (3.648112876441578, 20, 22), (3.650051389180301, 8, 23), (3.667481765821636, 14, 23), (3.6980664818982665, 17, 24), (3.792955142257148, 14, 15), (3.830985970796699, 17, 20), (3.8381635150976563, 7, 17), (3.8660836888356886, 3, 6), (3.88057650209476, 0, 10), (3.8815446666107274, 18, 25), (3.8865584011749648, 5, 19), (3.895070327375771, 8, 21), (3.905622443248538, 2, 5), (3.9120615355808326, 0, 7), (3.96059230681634, 8, 10), (3.982827542711841, 0, 16), (4.001893922947541, 7, 11), (4.003934463688903, 6, 22), (4.00986833283281, 0, 24), (4.015846277600932, 0, 6), (4.064784159713256, 6, 20), (4.079008101576092, 9, 15), (4.085518861696584, 10, 12), (4.085929400980456, 3, 12), (4.119748311432924, 6, 14), (4.125963307950302, 7, 12), (4.131148629484805, 2, 18), (4.143859656793796, 9, 21), (4.150049689100727, 11, 24), (4.162075821469023, 9, 23), (4.1731430020562055, 1, 23), (4.185058918924211, 17, 19), (4.253027604109994, 8, 20), (4.2661759459376, 12, 15), (4.322764326309356, 11, 13), (4.359820934656835, 5, 21), (4.36324532175667, 6, 21), (4.367060226962038, 5, 15), (4.375749678095554, 0, 8), (4.37775783307109, 0, 20), (4.3942281110607855, 1, 16), (4.401429573896292, 1, 15), (4.401429667216539, 16, 17), (4.465474096725329, 12, 21), (4.467915516146843, 6, 16), (4.4689774849631165, 14, 20), (4.47222048593539, 5, 20), (4.481833931403168, 6, 12), (4.521037160999228, 16, 23), (4.529317625743097, 8, 17), (4.568825844128019, 6, 19), (4.598606880931805, 0, 9), (4.623550935954804, 15, 16), (4.6352218027363135, 3, 10), (4.68301448453738, 5, 23), (4.688027136627815, 3, 22), (4.699813166639882, 9, 20), (4.705577771300183, 10, 22), (4.734082883909135, 17, 22), (4.769189206065441, 12, 24), (4.782883688667644, 10, 14), (4.782883688667644, 13, 14), (4.790716536239136, 16, 20), (4.815244458713966, 2, 23), (4.840604999561364, 0, 21), (4.852297970250509, 6, 13), (4.866856420813416, 9, 10), (4.876739378337856, 21, 22), (4.899041424719925, 15, 22), (4.90779282466864, 22, 23), (4.951124828199568, 1, 6), (4.984549301523895, 1, 12), (5.043138553163, 6, 18), (5.080204670865097, 6, 24), (5.101154775192579, 5, 24), (5.113431846162941, 0, 19), (5.127630111784468, 6, 7), (5.130582339623548, 11, 23), (5.149244425628588, 9, 17), (5.149293285146158, 5, 17), (5.22325114047293, 2, 20), (5.240112750433508, 3, 20), (5.242640666740577, 3, 13), (5.270837706280598, 0, 22), (5.278054996856731, 13, 16), (5.291502622129181, 2, 25), (5.304946951832261, 2, 22), (5.304946951832261, 13, 22), (5.332474203560016, 7, 14), (5.369211022848948, 2, 17), (5.388931438657519, 5, 7), (5.434390412748942, 12, 19), (5.464235037939374, 10, 16), (5.479069564109438, 14, 21), (5.52563110949126, 8, 12), (5.528099671545803, 14, 24), (5.5677643628300215, 16, 21), (5.584153883409815, 5, 22), (5.590517567203119, 5, 13), (5.613797124484201, 11, 17), (5.635398307124009, 3, 24), (5.656854249492381, 11, 19), (5.669848785051797, 0, 11), (5.6719990175601405, 11, 21), (5.67535369142605, 3, 7), (5.675662249507845, 1, 10), (5.703735544200627, 9, 12), (5.71764500203349, 2, 12), (5.771802840666905, 3, 21), (5.791786908381317, 6, 25), (5.799854057881404, 11, 15), (5.81083965336458, 19, 22), (5.820152024988017, 6, 8), (5.8317319814199005, 2, 4), (5.835728601356442, 2, 15), (5.839977414831645, 5, 8), (5.845645632024165, 2, 21), (5.889506891142312, 10, 11), (5.927949597557555, 7, 22), (5.932370207866713, 1, 22), (6.017735618194642, 1, 13), (6.056920952168088, 2, 19), (6.087600018776694, 0, 2), (6.116666410093156, 7, 16), (6.122522450800983, 2, 3), (6.1243428959998765, 14, 19), (6.1693289653006715, 5, 18), (6.175958400260999, 22, 25), (6.207463182184621, 22, 24), (6.220246327058001, 12, 25), (6.235060254666591, 2, 14), (6.244212343040575, 0, 5), (6.250311112168272, 10, 18), (6.252059164892923, 11, 20), (6.283731424313536, 3, 19), (6.288081993267387, 16, 24), (6.362804461313063, 3, 8), (6.362804504348855, 16, 19), (6.3645562223759296, 8, 14), (6.417677087324943, 23, 25), (6.417753672925978, 3, 5), (6.418063055943195, 1, 24), (6.421798895611531, 1, 20), (6.4445790520543405, 11, 12), (6.469765416909976, 2, 16), (6.47102538968687, 6, 9), (6.48074069840786, 2, 13), (6.48074069840786, 10, 25), (6.4901741651684945, 18, 22), (6.493964340462857, 21, 25), (6.506537572951515, 1, 7), (6.51900901130036, 2, 7), (6.519783424914063, 9, 14), (6.576928223870398, 5, 12), (6.614072852952884, 5, 14), (6.6411848495388, 5, 9), (6.697932181024764, 1, 21), (6.709320247238359, 18, 23), (6.717659065206948, 2, 24), (6.771843511913452, 16, 25), (6.818011485242996, 3, 9), (6.835590955532766, 5, 16), (6.8762164361261755, 20, 25), (6.987734039578398, 17, 25), (6.989606193708787, 1, 8), (7.0710678118654755, 13, 25), (7.086123197524049, 1, 2), (7.125453355923639, 16, 18), (7.189317868950627, 2, 8), (7.201013452503212, 1, 19), (7.2162951710885785, 18, 20), (7.240722420806826, 12, 18), (7.258532318644919, 19, 25), (7.281792966974647, 18, 21), (7.313363190800938, 8, 16), (7.313363190800938, 9, 16), (7.40039576489759, 8, 22), (7.406388778826847, 1, 9), (7.407832097133615, 14, 25), (7.417973716011397, 4, 18), (7.4332958147248975, 5, 25), (7.438129760852192, 18, 19), (7.544123486097521, 1, 5), (7.553732454132196, 0, 25), (7.581904814286804, 3, 11), (7.602071977056552, 14, 18), (7.6658892163345405, 9, 22), (7.672950514239754, 6, 11), (7.690182803838489, 17, 18), (7.713261074480635, 0, 18), (7.736575130064481, 1, 11), (7.842676578042994, 11, 14), (7.875463918020496, 15, 18), (7.8775458303135, 15, 25), (7.9414349458327855, 13, 18), (7.967765129605797, 3, 25), (7.980369128105089, 2, 9), (8.03103221817814, 7, 25), (8.115307210271776, 4, 6), (8.174742088811922, 3, 18), (8.312961939389817, 5, 11), (8.314261441424433, 24, 25), (8.438032964025735, 7, 18), (8.60289067473192, 18, 24), (8.61470498453014, 1, 25), (8.642123085261675, 2, 11), (8.669882528572723, 4, 5), (8.859191902231382, 11, 16), (8.926717841447257, 8, 25), (8.94478048378582, 4, 25), (9.038046880870489, 9, 25), (9.055887471866889, 4, 10), (9.058962427130702, 1, 18), (9.073355186438757, 8, 18), (9.152368954380838, 11, 22), (9.291166468171534, 9, 18), (9.417339933378893, 11, 25), (9.512609028678119, 4, 22), (10.44940588804216, 4, 20), (10.463162247589974, 4, 23), (10.551102991596428, 11, 18), (10.627730957597764, 4, 12), (10.644156820869837, 4, 17), (10.692822840360837, 3, 4), (10.934788590666265, 4, 14), (10.979572308740792, 4, 16), (11.331297241298651, 4, 15), (11.383093118873102, 0, 4), (11.406211156972343, 4, 21), (11.45658386044323, 1, 4), (11.684919927931533, 4, 19), (12.098430688400168, 4, 7), (12.165898976364865, 4, 13), (12.372068260830382, 4, 24), (12.78817241525041, 4, 8), (13.694427834786362, 4, 9), (13.947664812511501, 4, 11)] \n",
            "\n",
            "min distance is between 7 th letter and 24 th letter. min distance is 1.0591611238255987 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flyYvM5QavBK",
        "colab_type": "text"
      },
      "source": [
        "The algorithm above obtains a feature vector for all the letters of the Latin alphabet. By using a sorted list we notice that the smallest distance is between letter H and W. Thus, we have classified all of our letters since the minimum distance is above 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfV7jhOavBP",
        "colab_type": "text"
      },
      "source": [
        "### Our experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R-kfIXttzIUg"
      },
      "source": [
        "Next, we will create a test feature block which will contain all the features we have already implented ie. Probing from upper left, scanning from left to right, scanning from right to left, etc. This will allow us to now test a new string of 100 zeros and ones against our feature matrix so that we can compare our test to our feature matrix and find out the best fitted letter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImtdLt3mavBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert new test input list of 01010101... into feature vector.\n",
        "def get_feature_vec(test): # 'test' stands for the 01010101... input.\n",
        "    #run all the scans, get the life-death pairs.\n",
        "    letter= np.full((10,10),100)\n",
        "   #test 1 probing upper left\n",
        "    for k in range(1,100):\n",
        "        if test[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max((k-1)%10, int((k-1)/10))\n",
        "    PULscan = lower_star_img(letter)\n",
        "    #test 2 left to right\n",
        "    for j in range(1,100):\n",
        "        if test[j]==1.0:\n",
        "            row=int((j-1)/10)\n",
        "            column= (j-1)%10\n",
        "            letter[row,column]= j%10\n",
        "    LRscan = lower_star_img(letter)\n",
        "    #test 3 right to left\n",
        "    for k in range(1,100):\n",
        "        if test[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= 10 - (k-1)%10\n",
        "    RLscan = lower_star_img(letter)\n",
        "    #test 4 probing from bottom right\n",
        "    for k in range(1,100):\n",
        "        if test[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max(9-(k-1)%10,9-int((k-1)/10))  \n",
        "    PLBscan = lower_star_img(letter)\n",
        "    #test 5 probing from upper right to lower left \n",
        "    for k in range(1,100):       \n",
        "        if test[k]==1.0:\n",
        "            row=int((k-1)/10)\n",
        "            column= (k-1)%10\n",
        "            letter[row,column]= max(9-(k-1)%10,int((k-1)/10))  \n",
        "    PURscan = lower_star_img(letter) \n",
        "    #test 6\n",
        "    newletter = test[1:].reshape(10,10)\n",
        "    coordinates = np.argwhere(newletter == 1)\n",
        "    dgms = ripser(coordinates)['dgms']\n",
        "    h1_test = dgms[1]\n",
        "    test_result = (PULscan, LRscan, RLscan, PLBscan, PURscan, h1_test)\n",
        "    #change all infini to 100\n",
        "    for j in range(6):\n",
        "        len_j = len(test_result[j])\n",
        "        for k in range(len_j):\n",
        "            if str(test_result[j][k][1])=='inf':\n",
        "                test_result[j][k][1]=100\n",
        "    #get the feature vector\n",
        "    test_feature = np.zeros(6)\n",
        "    for j in range(6):\n",
        "        len_j = len(test_result[j])\n",
        "        sum_j = 0\n",
        "        for k in range(len_j):\n",
        "            sum_j = sum_j+ (test_result[j][k][1]-test_result[j][k][0])\n",
        "            test_feature[j] = sum_j\n",
        "            \n",
        "    return test_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SofKsq3FavBS",
        "colab_type": "text"
      },
      "source": [
        "To determine that we do get the correct feature vector we first take a known string of zeros and ones. We will compare this to our feature matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA6wLb166jAP",
        "colab_type": "code",
        "outputId": "6561d354-33d4-45b0-8ab2-32b7ecf6e512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "#Here is where we make up a test list. We will be using the letter I (the 8th letter) in 10X10 grid.\n",
        "New_I = [8,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "New_I = np.asarray(New_I)\n",
        "plt.imshow(New_I[1:].reshape(10,10))\n",
        "print(New_I)\n",
        "feature_vect = get_feature_vec(New_I)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[8])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "feature vect of input list is: [97. 97. 96. 96. 96.  0.]\n",
            "standard feature vect of I is: [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 8 th letter \n",
            " the closest distance is: 0.0 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njFpqxqilZv4NgcIHD1OJ/kQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4STMYtvK0rH",
        "colab_type": "text"
      },
      "source": [
        "Notice that our algorithm does work when determining an exact string from our orignal matrix. Now we will take out four points from the string and see if our algorithm is able to determine that it is the letter I. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a4zPnDt-Ms4",
        "colab_type": "code",
        "outputId": "b1d8c4e1-95f1-4c95-c926-e170864c6e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "#Here is where we manual take out 4 points from the letter I\n",
        "New_I = [8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "New_I = np.asarray(New_I)\n",
        "plt.imshow(New_I[1:].reshape(10,10))\n",
        "print(New_I)\n",
        "feature_vect = get_feature_vec(New_I)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[8])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "feature vect of input list is: [96. 95. 96. 96. 96.  0.]\n",
            "standard feature vect of I is: [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 8 th letter \n",
            " the closest distance is: 2.23606797749979 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBnQEoOIKiyUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itfkeOHCavBa",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the picture above, we have removed some of the points in I. However, this did not affect our algorithm as we are still getting that the best fitted letter is the 8th letter (I). Now we will randomly remove points to make sure that our algrothim is as robust as we believe it is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6ZYIH6Zlgc",
        "colab_type": "code",
        "outputId": "96fdfd87-0ebb-43be-a7ab-2a5142d80af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "def takeoff_pts(letter_one_line, num_of_pts): #input the 010101001.. sequece of a letter, and the number of points you want to take of from this letter, say 2, or 5 etc.\n",
        "    one_list = []\n",
        "    l = len(letter_one_line)\n",
        "    for i in range(l):\n",
        "        if letter_one_line[i]==1:\n",
        "            one_list.append(i)\n",
        "\n",
        "    zero_posi = np.random.choice(one_list, size= num_of_pts)\n",
        "\n",
        "    minus_one = np.zeros(101)\n",
        "    \n",
        "    for i in zero_posi:\n",
        "        minus_one[i] = -1\n",
        "    \n",
        "    Noise_letter0 = letter_one_line + minus_one\n",
        "\n",
        "    return Noise_letter0\n",
        "\n",
        "letter_I = letters[8,:]\n",
        "\n",
        "\n",
        "Noise_I = takeoff_pts(letter_I, 3)\n",
        "\n",
        "Noise_feature0 = get_feature_vec(Noise_I)\n",
        "print('Feature vector of Noise I:\\n',Noise_feature0,'\\n','Original feature vector of I:\\n',vec_mtx[8])\n",
        "\n",
        "Noise_I = np.asarray(Noise_I)\n",
        "plt.imshow(Noise_I[1:].reshape(10,10))\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(Noise_feature0 - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature vector of Noise I:\n",
            " [286. 287. 284. 283. 284.   0.] \n",
            " Original feature vector of I:\n",
            " [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 4 th letter \n",
            " the closest distance is: 412.0091471723925 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2MLdv9-avBW",
        "colab_type": "text"
      },
      "source": [
        "Now we are not getting an accurate best fitted letter. To see exactly how the result is affected by the position of noise point, we will again manually remove a few points and examine its effect. Lets take off the upper bar of I."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnYz2DRFfIvj",
        "colab_type": "code",
        "outputId": "a6850b8b-b37b-47a5-af62-af7ad128feab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "New_I = [8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "New_I = np.asarray(New_I)\n",
        "plt.imshow(New_I[1:].reshape(10,10))\n",
        "print(New_I)\n",
        "feature_vect = get_feature_vec(New_I)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[8])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "feature vect of input list is: [96. 96. 95. 96. 95.  0.]\n",
            "standard feature vect of I is: [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 8 th letter \n",
            " the closest distance is: 2.0 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAJc0lEQVR4nO3dzY9dBR2H8edrp7S2GjTRDS2RLgym\nMVHMRHlJXFASQY1sXGACiWy6EUVjYtCN/wAxuiAkDS8bCCwKC2IIYHxZuGkcShNtCwkBhPIS60I0\nJLYl/lzMmNSW9p65vYcz88vzSUh67z29fEPm4dx7eqeTqkJSHx+ZeoCkxTJqqRmjlpoxaqkZo5aa\nWRrjSS/LttrOzjGeWhLwb97jdJ3KBz02StTb2clXsm+Mp5YEHKrfXvAxX35LzRi11IxRS80YtdSM\nUUvNGLXUzKCok9yc5KUkLye5Z+xRkuY3M+okW4D7gFuAvcB3kuwde5ik+Qw5U38ZeLmqXqmq08Dj\nwK3jzpI0ryFR7wLeOOv2ibX7/k+S/UlWkqyc4dSi9klap4VdKKuqA1W1XFXLW9m2qKeVtE5Don4T\nuPKs27vX7pO0AQ2J+k/AZ5PsSXIZcBvw1LizJM1r5ndpVdX7Se4CngW2AA9V1dHRl0may6Bvvayq\np4GnR94iaQH8RJnUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFL\nzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11Mygn6WljePZt46M\n8rxfu+KLozyvPnyeqaVmjFpqxqilZoxaasaopWaMWmrGqKVmZkad5Mokv09yLMnRJHd/GMMkzWfI\nh0/eB35cVYeTfBx4PslvqurYyNskzWHmmbqq3q6qw2u//hdwHNg19jBJ81nXx0STXAVcAxz6gMf2\nA/sBtrNjAdMkzWPwhbIkHwOeAH5YVf889/GqOlBVy1W1vJVti9woaR0GRZ1kK6tBP1pVT447SdKl\nGHL1O8CDwPGq+sX4kyRdiiFn6huAO4AbkxxZ++frI++SNKeZF8qq6o9APoQtkhbAT5RJzRi11IxR\nS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFL\nzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdSMUUvNGLXUjFFLzRi11IxRS80YtdTM4KiT\nbEnyQpJfjzlI0qVZz5n6buD4WEMkLcagqJPsBr4BPDDuHEmXauiZ+pfAT4D/XOiAJPuTrCRZOcOp\nhYyTtH4zo07yTeBvVfX8xY6rqgNVtVxVy1vZtrCBktZnyJn6BuBbSV4DHgduTPLIqKskzW1m1FX1\n06raXVVXAbcBv6uq20dfJmku/jm11MzSeg6uqj8AfxhliaSF8EwtNWPUUjNGLTVj1FIzRi01Y9RS\nM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIz\nRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01MyjqJJ9IcjDJi0mO\nJ7lu7GGS5rM08LhfAc9U1beTXAbsGHGTpEswM+oklwNfBb4LUFWngdPjzpI0ryEvv/cAJ4GHk7yQ\n5IEkO889KMn+JCtJVs5wauFDJQ0zJOol4EvA/VV1DfAecM+5B1XVgaparqrlrWxb8ExJQw2J+gRw\noqoOrd0+yGrkkjagmVFX1TvAG0muXrtrH3Bs1FWS5jb06vf3gUfXrny/Atw53iRJl2JQ1FV1BFge\neYukBfATZVIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RS\nM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01M/RnaWmdnn3ryNQT1mWsvV+7\n4oujPK8uzDO11IxRS80YtdSMUUvNGLXUjFFLzRi11MygqJP8KMnRJH9J8liS7WMPkzSfmVEn2QX8\nAFiuqs8DW4Dbxh4maT5DX34vAR9NsgTsAN4ab5KkSzEz6qp6E7gXeB14G3i3qp4797gk+5OsJFk5\nw6nFL5U0yJCX358EbgX2AFcAO5Pcfu5xVXWgqparankr2xa/VNIgQ15+3wS8WlUnq+oM8CRw/biz\nJM1rSNSvA9cm2ZEkwD7g+LizJM1ryHvqQ8BB4DDw57Xfc2DkXZLmNOj7qavq58DPR94iaQH8RJnU\njFFLzRi11IxRS80YtdSMf5voSPxbNDUVz9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNG\nLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjOpqsU/aXIS+OuAQz8F/H3h\nA8azmfZupq2wufZuhK2fqapPf9ADo0Q9VJKVqlqebMA6baa9m2krbK69G32rL7+lZoxaambqqDfb\nD6/fTHs301bYXHs39NZJ31NLWrypz9SSFsyopWYmizrJzUleSvJyknum2jFLkiuT/D7JsSRHk9w9\n9aYhkmxJ8kKSX0+95WKSfCLJwSQvJjme5LqpN11Mkh+tfR38JcljSbZPvelck0SdZAtwH3ALsBf4\nTpK9U2wZ4H3gx1W1F7gW+N4G3nq2u4HjU48Y4FfAM1X1OeALbODNSXYBPwCWq+rzwBbgtmlXnW+q\nM/WXgZer6pWqOg08Dtw60ZaLqqq3q+rw2q//xeoX3a5pV11ckt3AN4AHpt5yMUkuB74KPAhQVaer\n6h/TrpppCfhokiVgB/DWxHvOM1XUu4A3zrp9gg0eCkCSq4BrgEPTLpnpl8BPgP9MPWSGPcBJ4OG1\ntwoPJNk59agLqao3gXuB14G3gXer6rlpV53PC2UDJfkY8ATww6r659R7LiTJN4G/VdXzU28ZYAn4\nEnB/VV0DvAds5Osrn2T1FeUe4ApgZ5Lbp111vqmifhO48qzbu9fu25CSbGU16Eer6smp98xwA/Ct\nJK+x+rbmxiSPTDvpgk4AJ6rqf698DrIa+UZ1E/BqVZ2sqjPAk8D1E286z1RR/wn4bJI9SS5j9WLD\nUxNtuagkYfU93/Gq+sXUe2apqp9W1e6quorV/66/q6oNdzYBqKp3gDeSXL121z7g2ISTZnkduDbJ\njrWvi31swAt7S1P8S6vq/SR3Ac+yegXxoao6OsWWAW4A7gD+nOTI2n0/q6qnJ9zUyfeBR9f+5/4K\ncOfEey6oqg4lOQgcZvVPRV5gA35k1I+JSs14oUxqxqilZoxaasaopWaMWmrGqKVmjFpq5r+/+AN8\nOdMHKgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp_p6f68fmuH",
        "colab_type": "text"
      },
      "source": [
        "Again, the best fitted letter is I so let's remove a center point and see what happens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQPU6FFQfmIe",
        "colab_type": "code",
        "outputId": "671b7a9c-aa02-4cc2-8ac4-42ada1d85c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "#Here is where we make up a test list. We will be using the letter I (the 8th letter) in 10X10 grid.\n",
        "New_I = [8,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "New_I = np.asarray(New_I)\n",
        "plt.imshow(New_I[1:].reshape(10,10))\n",
        "print(New_I)\n",
        "feature_vect = get_feature_vec(New_I)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[8])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "feature vect of input list is: [191. 192. 190. 190. 190.   0.]\n",
            "standard feature vect of I is: [97. 97. 96. 96. 96.  0.]\n",
            "the best fitted letter is: 4 th letter \n",
            " the closest distance is: 201.45604322958934 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqti6epOQm7u",
        "colab_type": "text"
      },
      "source": [
        "From above result, we see if we took a point at center of I off, it changes drastically. The feature vector of new I has a norm greater than most of the vectors in feature matrix, so the new result is letter E, since it has the greatest norm in our feature map, therefore closer to our new letter I."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvKTLJSGhACF",
        "colab_type": "code",
        "outputId": "8e872924-2420-412e-e41b-f9ceb1c504f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "new_A = [2,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1]\n",
        "new_A = np.asarray(new_A)\n",
        "#new_A = letters[0]\n",
        "plt.imshow(new_A[1:].reshape(10,10))\n",
        "print(new_A)\n",
        "feature_vect = get_feature_vec(new_A)\n",
        "\n",
        "\n",
        "print('feature vect of input list is:',feature_vect)\n",
        "print('standard feature vect of I is:', vec_mtx[0])\n",
        "\n",
        "#compare the newly obtained test feature vector, to each of the feature vector stored in matrix.\n",
        "test_dis = []\n",
        "for i in range(len(vec_mtx)):\n",
        "    # compare feature_vect with each of the vect in vec_mtx, see which distance=norm(feature_vect - vect) is smalletst.\n",
        "    d = np.linalg.norm(feature_vect - vec_mtx[i])\n",
        "    test_dis.append((d,i))\n",
        "    \n",
        "#then find the which vector has the smallest distance with input test vector, choose it as our best fitted letter.\n",
        "sorted_result = sorted(test_dis,key=getkey)\n",
        "print('the best fitted letter is:',sorted_result[0][1],'th letter','\\n','the closest distance is:',sorted_result[0][0],'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "feature vect of input list is: [ 96.        107.         99.         99.         97.          2.3005631]\n",
            "standard feature vect of I is: [97.         98.         97.         98.         97.          3.48528123]\n",
            "the best fitted letter is: 18 th letter \n",
            " the closest distance is: 7.140882049405402 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9C3EcnEZAjoi"
      },
      "source": [
        "### Advantages of the classification system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hyFz1nKY_kb",
        "colab_type": "text"
      },
      "source": [
        "The main advantage of our classification system is that we have successfully differentiated between letters. We know that this is true since the minimum difference between each letter pair is greater than zero. So, each letter has a unique feature vector. Another advantage of our classification system is that we are able to determine exact letters with no points missing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGTTvSrtavBb",
        "colab_type": "text"
      },
      "source": [
        "### Criticism of the classification system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3un4zHGavBc",
        "colab_type": "text"
      },
      "source": [
        "We observe that the results are not robust since eliminating random points from the letter 'I' fails to give the expected best fitted letter\n",
        "\n",
        "To take a closer look of what happend, we discarded some points manually from the letter to observe how  the postion of noise block affects the result. \n",
        "The letter 'I' is not affected by eliminating some points in the upper bar portion of the letter.\n",
        "Whereas, by having some points eliminated at the center of the letter I, the best fitted letter observed is E, as it has the largest norm in our feature map.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59bDQg9aeEY9",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "In conclusion, we format the letters on the image into vectors and then scan them for multi-dimensional and multi-directional algorithms, even for noise removal. Image vectorization is a good way to deal with image recognition in topological data analysis, machine learning, deep learning and even in reinforcement learning. I believe that if we have more test data sets, our results will become more and more accurate. If the final algorithm of our algorithm is not ideal for the recognition rate of new handwritten letters, the reasons may be as follows: First, the image feature extraction is too simple, the edges of the image are blank, and the center positions of the letters in the image may not all correspond; Then, the sample size is small, and each character has only one training sample. The real training requires massive data. Of course, we can also try to improve the classification recognition rate with other learning algorithms, such as KNN algorithm, SVM algorithm, logistic regression, neural network, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwp3CnlpekDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}